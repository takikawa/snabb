-- -*- lua -*-
--
-- SipHash is a family of hash functions, parameterized by the number of
-- rounds that run per 8-byte input block and the number of rounds that
-- run at the end.  SipHash was designed to be good for short inputs and
-- to resist "hash flooding" attacks, where users attack a hash table
-- with many inputs that map to the same area of the hash table.
--
-- We provide implementations of SipHash with a normal scalar DynASM
-- backend as well as parallel SSE and AVX2 backends.
--
-- Because we use SipHash as a hash function for fixed-sized inputs, we
-- can simplify processing of the tail word.  This simplification is
-- enabled unless a true value for "as_specified" is passed.
--
-- This implementation was based on the reference implementation.  See
-- https://131002.net/siphash/ for more details.

module(..., package.seeall)

local bit  = require("bit")
local dasm = require("dasm")
local ffi  = require("ffi")
local S    = require("syscall")

local debug = false

local cpuinfo = require('core.lib').readfile("/proc/cpuinfo", "*a")
assert(cpuinfo, "failed to read /proc/cpuinfo for hardware check")
local have_avx2 = cpuinfo:match("avx2")
local have_sse2 = cpuinfo:match("sse2")

|.arch x64
|.actionlist actions

__anchor = {}
local function finish (name, prototype, Dst)
   local mcode, size = Dst:build()
   table.insert(__anchor, mcode)
   if debug then
      print("mcode dump: "..name)
      dasm.dump(mcode, size)
   end
   return ffi.cast(prototype, mcode)
end

local function random_bytes(count)
   local bytes = ffi.new('uint8_t[?]', count)
   local f = S.open('/dev/urandom', 'rdonly')
   local written = 0
   while written < count do
      written = written + assert(f:read(bytes, count-written))
   end
   return bytes
end

function random_sip_hash_key()
   return random_bytes(16)
end

function reference_sip_hash_key()
   return ffi.new('uint8_t[16]',
                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)
end

local function load_initial_state(key)
   -- Initial state constants.
   local state = ffi.new('uint64_t[4]',
                         { 0x736f6d6570736575ULL,
                           0x646f72616e646f6dULL,
                           0x6c7967656e657261ULL,
                           0x7465646279746573ULL })

   -- Mix key into state constants.
   key = ffi.cast('uint64_t*', key or reference_sip_hash_key())
   state[0] = bit.bxor(state[0], key[0])
   state[1] = bit.bxor(state[1], key[1])
   state[2] = bit.bxor(state[2], key[0])
   state[3] = bit.bxor(state[3], key[1])

   return state
end

-- Scalar x86-64 backend for the SipHash implementation.
local function X86_64()
   local asm = {}
   local Dst
   local allregs = {"rax", "rcx", "rdx", "rbx", "rsp", "rbp", "rsi", "rdi",
                    "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"}
   local regnums = {}
   for i,reg in ipairs(allregs) do regnums[reg] = i-1 end
   -- Map value indexes from the program to registers.  Don't allocate
   -- to rdi; it's the input data.
   local scratchregs = {"r8", "r9", "r10", "r11", "rax", "rcx", "rdx", "rsi"}
   local function regnum(n) return assert(regnums[assert(scratchregs[n+1])]) end

   function asm.init(key)
      local initial_state = load_initial_state(key)
      table.insert(__anchor, initial_state)
      | mov64 Rq(regnum(0)), initial_state[0]
      | mov64 Rq(regnum(1)), initial_state[1]
      | mov64 Rq(regnum(2)), initial_state[2]
      | mov64 Rq(regnum(3)), initial_state[3]
   end
   function asm.add(dst, other)
      | add Rq(regnum(dst)), Rq(regnum(other))
   end
   function asm.shl(reg, bits)
      | shl Rq(regnum(reg)), bits
   end
   function asm.ior(dst, other)
      | or Rq(regnum(dst)), Rq(regnum(other))
   end
   function asm.xor(dst, other)
      | xor Rq(regnum(dst)), Rq(regnum(other))
   end
   function asm.rol(reg, bits)
      | rol Rq(regnum(reg)), bits
   end
   function asm.load_u64_and_advance(dst)
      | mov Rq(regnum(dst)), [rdi]
      | add rdi, 8
   end
   function asm.load_u32_and_advance(dst)
      -- Automatically zero-extending.
      | mov Rd(regnum(dst)), [rdi]
      | add rdi, 4
   end
   function asm.load_u16_and_advance(dst)
      | movzx Rq(regnum(dst)), word [rdi]
      | add rdi, 2
   end
   function asm.load_u8_and_advance(dst)
      | movzx Rq(regnum(dst)), byte [rdi]
      | add rdi, 1
   end
   function asm.load_imm8(dst, imm)
      | xor Rq(regnum(dst)), Rq(regnum(dst))
      if imm ~= 0 then
         | mov Rb(regnum(dst)), imm
      end
   end
   function asm.ret(reg)
      if regnum(reg) ~= regnums["rax"] then
         | mov rax, Rq(regnum(reg))
      end
      | shr rax, 32
      | ret
   end
   function asm:assemble(name, gen)
      Dst = dasm.new(actions)
      gen(self)
      return finish(name.."_x86_64",
                    ffi.typeof("uint32_t (*)(uint8_t *)"),
                    Dst)
   end
   return asm
end

-- Parallel SSE backend for the SipHash implementation that can hash two
-- inputs at once.
local function SSE(stride)
   local asm = {}
   local Dst
   function asm.init(key)
      local initial_state = load_initial_state(key)
      table.insert(__anchor, initial_state)
      | mov rax, initial_state
      | movddup xmm0, [rax]
      | movddup xmm1, [rax+8]
      | movddup xmm2, [rax+16]
      | movddup xmm3, [rax+24]
   end
   function asm.add(dst, other)
      | paddq xmm(dst), xmm(other)
   end
   function asm.shl(reg, bits)
      | psllq xmm(reg), bits
   end
   function asm.ior(dst, other)
      | por xmm(dst), xmm(other)
   end
   function asm.xor(dst, other)
      | pxor xmm(dst), xmm(other)
   end
   function asm.rol(reg, bits)
      | movupd xmm5, xmm(reg)
      | psllq xmm5, bits
      | psrlq xmm(reg), (64 - bits)
      | por xmm(reg), xmm5
   end
   function asm.load_u64_and_advance(dst)
      | movlpd xmm(dst), qword [rdi]
      | movhpd xmm(dst), qword [rdi+stride]
      | add rdi, 8
   end
   function asm.load_u32_and_advance(dst)
      | movss xmm(dst), dword [rdi]
      | pinsrd xmm(dst), dword [rdi + stride], 2
      | add rdi, 4
   end
   function asm.load_u16_and_advance(dst)
      | pxor xmm(dst), xmm(dst)
      | pinsrw xmm(dst), word [rdi], 0
      | pinsrw xmm(dst), word [rdi + stride], 4
      | add rdi, 2
   end
   function asm.load_u8_and_advance(dst)
      | pxor xmm(dst), xmm(dst)
      | pinsrb xmm(dst), byte [rdi], 0
      | pinsrb xmm(dst), byte [rdi + stride], 8
      | add rdi, 1
   end
   function asm.load_imm8(dst, imm)
      | xor rax, rax
      if imm ~= 0 then
         | mov al, imm
      end
      | pinsrq xmm(dst), rax, 0
      | pinsrq xmm(dst), rax, 1
   end
   function asm.ret(reg)
      -- Extract high 32 bits from each 64-bit value to uint32_t[2] in
      -- rsi.
      | pextrd dword [rsi], xmm(reg), 1
      | pextrd dword [rsi+4], xmm(reg), 3
      | ret
   end
   function asm:assemble(name, gen)
      Dst = dasm.new(actions)
      gen(self)
      return finish(name.."_x2",
                    ffi.typeof("void (*)(uint8_t *, uint32_t *)"),
                    Dst)
   end
   return asm
end

-- Parallel AVX2 backend for the SipHash implementation that can hash
-- four inputs at once.
local function AVX2(stride)
   local asm = {}
   local Dst
   function asm.init(key)
      local initial_state = load_initial_state(key)
      table.insert(__anchor, initial_state)
      | vzeroupper
      | mov rax, initial_state
      | vbroadcastsd ymm0, qword [rax]
      | vbroadcastsd ymm1, qword [rax+8]
      | vbroadcastsd ymm2, qword [rax+16]
      | vbroadcastsd ymm3, qword [rax+24]
   end
   function asm.add(dst, other)
      | vpaddq ymm(dst), ymm(dst), ymm(other)
   end
   function asm.shl(reg, bits)
      | vpsllq ymm(reg), ymm(reg), bits
   end
   function asm.ior(dst, other)
      | vpor ymm(dst), ymm(dst), ymm(other)
   end
   function asm.xor(dst, other)
      | vpxor ymm(dst), ymm(dst), ymm(other)
   end
   function asm.rol(reg, bits)
      | vpsllq ymm5, ymm(reg), bits
      | vpsrlq ymm(reg), ymm(reg), (64 - bits)
      | vpor ymm(reg), ymm(reg), ymm5
   end
   function asm.load_u64_and_advance(dst)
      -- TODO: Use a parallel load here.
      assert(dst > 5)
      | vpinsrq xmm(dst), xmm(dst), qword [rdi], 0
      | vpinsrq xmm(dst), xmm(dst), qword [rdi+stride*1], 1
      | vpinsrq xmm5, xmm5, qword [rdi+stride*2], 0
      | vpinsrq xmm5, xmm5, qword [rdi+stride*3], 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 8
   end
   function asm.load_u32_and_advance(dst)
      assert(dst > 5)
      | mov eax, [rdi]
      | mov edx, [rdi+stride]
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm5, xmm5, rdx, 0
      | mov eax, [rdi+stride*2]
      | mov edx, [rdi+stride*3]
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vpinsrq xmm5, xmm5, rdx, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 4
   end
   function asm.load_u16_and_advance(dst)
      assert(dst > 5)
      | movzx rax, word [rdi]
      | movzx rdx, word [rdi+stride]
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm5, xmm5, rdx, 0
      | movzx rax, word [rdi+stride*2]
      | movzx rdx, word [rdi+stride*3]
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vpinsrq xmm5, xmm5, rdx, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 2
   end
   function asm.load_u8_and_advance(dst)
      assert(dst > 5)
      | movzx rax, byte [rdi]
      | movzx rdx, byte [rdi+stride]
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm5, xmm5, rdx, 0
      | movzx rax, byte [rdi+stride*2]
      | movzx rdx, byte [rdi+stride*3]
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vpinsrq xmm5, xmm5, rdx, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 1
   end
   function asm.load_imm8(dst, imm)
      | xor rax, rax
      if imm ~= 0 then
         | mov al, imm
      end
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm(dst), 1
   end
   local function imm8_control(a, b, c, d)
      assert(bit.band(bit.bor(a,b,c,d), 3) == bit.bor(a,b,c,d))
      return a + b*4 + c*4*4 + d*4*4*4
   end
   function asm.ret(reg)
      -- Extract high 32 bits from each 64-bit value.  For ridiculous
      -- reasons, the way we have to do this is to read in a set of
      -- indexes into the 32-bit words from memory, then permute, then
      -- write out.
      local control = ffi.new('uint32_t[8]', 1, 3, 5, 7, 0, 0, 0, 0)
      table.insert(__anchor, control)
      | mov rax, control
      | vmovdqu ymm5, [rax]
      | vpermd ymm(reg), ymm5, ymm(reg)
      | vmovdqu oword [rsi], xmm(reg)
      | vzeroupper
      | ret
   end
   function asm:assemble(name, gen)
      Dst = dasm.new(actions)
      gen(self)
      return finish(name.."_x4",
                    ffi.typeof("void (*)(uint8_t *, uint32_t *)"),
                    Dst)
   end
   return asm
end

-- Portable backend for the SipHash implementation to use as a
-- reference.
local function Simulator()
   local asm = {}
   local input, output
   local state
   function asm.init(key)
      local initial_state = load_initial_state(key)
      state = ffi.new('uint64_t[8]')
      for i=0,3 do state[i] = initial_state[i] end
   end
   function asm.add(dst, other)
      state[dst] = state[dst] + state[other]
   end
   function asm.shl(reg, bits)
      state[reg] = bit.lshift(state[reg], bits)
   end
   function asm.ior(dst, other)
      state[dst] = bit.bor(state[dst], state[other])
   end
   function asm.xor(dst, other)
      state[dst] = bit.bxor(state[dst], state[other])
   end
   function asm.rol(reg, bits)
      state[reg] = bit.rol(state[reg], bits)
   end
   function asm.load_u64_and_advance(dst)
      state[dst] = ffi.cast('uint64_t*', input)[0]
      input = input + 8
   end
   function asm.load_u32_and_advance(dst)
      state[dst] = ffi.cast('uint32_t*', input)[0]
      input = input + 4
   end
   function asm.load_u16_and_advance(dst)
      state[dst] = ffi.cast('uint16_t*', input)[0]
      input = input + 2
   end
   function asm.load_u8_and_advance(dst)
      state[dst] = input[0]
      input = input + 1
   end
   function asm.load_imm8(dst, imm)
      state[dst] = imm
   end
   function asm.ret(reg)
      output = tonumber(bit.rshift(state[reg], 32))
   end
   function asm:assemble(name, gen)
      return function(ptr)
         input, output = ffi.cast('uint8_t*', ptr), nil
         gen(self)
         input = nil
         return output
      end
   end
   return asm
end

local function make_sip_hash(assembler, size, stride, key, c, d, as_specified)
   c = c or 2
   d = d or 4
   function siphash(asm)
      -- Arguments:
      -- rdi: packed keys as pointer
      -- rsi: for parallel implementations, an output uint32_t[4]

      -- Working registers:
      -- Registers 0-3 map to SipHash variables v0-v3
      -- Registers 6 and 7 used as scratch.

      local function sipround()
         asm.add(0, 1)
         asm.rol(1, 13)
         asm.xor(1, 0)
         asm.rol(0, 32)
         asm.add(2, 3)
         asm.rol(3, 16)
         asm.xor(3, 2)
         asm.add(0, 3)
         asm.rol(3, 21)
         asm.xor(3, 0)
         asm.add(2, 1)
         asm.rol(1, 17)
         asm.xor(1, 2)
         asm.rol(2, 32)
      end

      local function process(input)
         asm.xor(3, input)
         for i=1,c do sipround() end
         asm.xor(0, input)
      end

      -- Initialization phase.
      asm.init(key)

      -- Compression phase.
      for i=1,size/8 do
         asm.load_u64_and_advance(6)
         process(6)
      end
      -- Load tail word and process it.
      if as_specified then
         asm.load_imm8(6, bit.band(size, 0xff))
         asm.shl(6, 56)
         for i=1,size%8 do
            asm.load_u8_and_advance(7)
            if i > 1 then asm.shl(7, (i - 1) * 8) end
            asm.ior(6, 7)
         end
         process(6)
      elseif size%8 ~= 0 then
         -- Fixed-size simplification: no need to add in size byte, we
         -- can use different byte orders if it's more convenient, and
         -- we don't have to do anything at all if the size is a
         -- multiple of 8.
         if size%8 >= 4 then
            asm.load_u32_and_advance(6)
         else
            asm.xor(6, 6)
         end
         if size%4 >= 2 then
            asm.load_u16_and_advance(7)
            asm.shl(6, 16)
            asm.ior(6, 7)
         end
         if size%2 ~= 0 then
            asm.load_u8_and_advance(7)
            asm.shl(6, 8)
            asm.ior(6, 7)
         end
         process(6)
      end

      -- Finalization.
      asm.load_imm8(6, 0xff)
      asm.xor(2, 6)
      for i=1,d do sipround() end
      asm.xor(0, 1)
      asm.xor(2, 3)
      asm.xor(0, 2)
      asm.ret(0)
   end

   return assembler(stride):assemble("siphash_"..c.."_"..d, siphash)
end

function make_sip_hash_x1(size, key, c, d, as_specified)
   local stride = nil
   return make_sip_hash(X86_64, size, stride, key, c, d, as_specified)
end

function make_sip_hash_x2(size, stride, key, c, d, as_specified)
   if have_sse2 then
      return make_sip_hash(SSE, size, stride, key, c, d, as_specified)
   end
   local hash = make_sip_hash_x1(size, key, c, d, as_specified)
   return function(ptr, result)
      ptr = ffi.cast('uint8_t*', ptr)
      result = ffi.cast('uint32_t*', result)
      result[0] = hash(ptr)
      result[1] = hash(ptr + stride)
   end
end

function make_sip_hash_x4(size, stride, key, c, d, as_specified)
   if have_avx2 then
      return make_sip_hash(AVX2, size, stride, key, c, d, as_specified)
   end
   local hash = make_sip_hash_x2(size, stride, key, c, d, as_specified)
   return function(ptr, result)
      ptr = ffi.cast('uint8_t*', ptr)
      result = ffi.cast('uint32_t*', result)
      hash(ptr, result)
      hash(ptr + stride*2, result + 2)
   end
end

local function make_reference_sip_hash(size, key, c, d, as_specified)
   local stride = nil
   return make_sip_hash(Simulator, size, stride, key, c, d, as_specified)
end

function selftest()
   -- test that the lua and dasm versions produce the same output
   local function test(size, key, c, d, as_specified)
      local reference_hash = make_reference_sip_hash(size, key, c, d, as_specified)
      local hash_x1 = make_sip_hash_x1(size, key, c, d, as_specified)
      local stride = size
      local hash_x2 = make_sip_hash_x2(size, stride, key, c, d, as_specified)
      local hash_x4 = make_sip_hash_x4(size, stride, key, c, d, as_specified)

      for i=0, 255 do
         local buf = ffi.new("uint8_t[?]", size)
         for j=0, size-1 do buf[j] = (i + j) % size end

         local ref_result = reference_hash(buf)

         local x1_result = hash_x1(buf)

         local x2_buf = ffi.new("uint8_t[?]", size*2)
         local x2_result = ffi.new("uint32_t[2]")
         ffi.copy(x2_buf, buf, size); ffi.copy(x2_buf+size, buf, size)
         hash_x2(x2_buf, x2_result)

         local x4_buf = ffi.new("uint8_t[?]", size*4)
         ffi.copy(x4_buf, x2_buf, size*2); ffi.copy(x4_buf+size*2, x2_buf, size*2)
         local x4_result = ffi.new("uint32_t[4]")
         hash_x4(x4_buf, x4_result)

         if ref_result ~= x1_result then
            print(size, key, c, d, as_specified, ref_result, x1_result)
            error('scalar result does not match reference')
         end
         for elt=0,1 do
            if ref_result ~= x2_result[elt] then
               print(size, key, c, d, as_specified, ref_result, x2_result[elt])
               error('x2 result '..elt..' does not match reference')
            end
         end
         for elt=0,3 do
            if ref_result ~= x4_result[elt] then
               print(size, key, c, d, as_specified, ref_result, x4_result[elt])
               error('x4 result '..elt..' does not match reference')
            end
         end
      end
   end

   local key = reference_sip_hash_key()
   io.stdout:write("selftest: ")
   io.stdout:flush()
   for size=0,32 do
      io.stdout:write(".")
      io.stdout:flush()
      for c=0,2 do
         for d=0,4 do
            for _, as_specified in ipairs({true, false}) do
               test(size, key, c, d, as_specified)
            end
         end
      end
   end
   print("\nselftest ok")
end
