-- -*- lua -*-
-- Implementation of SipHash, based on the reference implementation.
-- See https://131002.net/siphash/ for more details.

module(..., package.seeall)

local bit  = require("bit")
local dasm = require("dasm")
local ffi  = require("ffi")

local debug = false

|.arch x64
|.actionlist actions

-- the definitions here (anchor, assemble, gen) are borrowed from lwaftr
-- (see multi_copy.lua)
__anchor = {}

local function finish (name, prototype, Dst)
   local mcode, size = Dst:build()
   table.insert(__anchor, mcode)
   if debug then
      print("mcode dump: "..name)
      dasm.dump(mcode, size)
   end
   return ffi.cast(prototype, mcode)
end

local function assemble (name, prototype, generator)
   local Dst = dasm.new(actions)
   generator(Dst)
   return finish(name, prototype, Dst)
end

-- SipHash is a family of hash functions, parameterized by the number
-- of rounds that run per 8-byte input block and the number of rounds
-- that run at the end.  Because we use SipHash as a hash function for
-- fixed-sized inputs, we can simplify processing of the tail word.
-- This simplification is enabled unless a true value for
-- "as_specified" is passed.

function random_sip_hash_key()
   error('unimplemented')
end

function reference_sip_hash_key()
   return ffi.new('uint8_t[16]',
                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)
end

local function load_initial_state(key)
   key = key or reference_sip_hash_key()

   local k0 = ffi.cast('uint64_t*', key)[0]
   local k1 = ffi.cast('uint64_t*', key)[1]

   -- Initial state constants.
   local v0 = 0x736f6d6570736575ULL
   local v1 = 0x646f72616e646f6dULL
   local v2 = 0x6c7967656e657261ULL
   local v3 = 0x7465646279746573ULL

   -- Mix key into state constants.
   v0 = bit.bxor(v0, k0)
   v1 = bit.bxor(v1, k1)
   v2 = bit.bxor(v2, k0)
   v3 = bit.bxor(v3, k1)

   return v0, v1, v2, v3
end

function make_sip_hash(size, key, c, d, as_specified)
   local v0, v1, v2, v3 = load_initial_state(key)
   c = c or 2
   d = d or 4

   function gen(Dst)
      -- Arguments:
      -- rdi: data as pointer

      -- Registers used:
      -- r8, r9, r10, r11 map to SipHash variables v0, v1, v2, v3
      -- rax, rdx: scratch registers

      -- Initialize state variables.
      | mov64 r8, v0
      | mov64 r9, v1
      | mov64 r10, v2
      | mov64 r11, v3

      local function sipround()
         | add r8, r9
         | rol r9, 13
         | xor r9, r8
         | rol r8, 32
         | add r10, r11
         | rol r11, 16
         | xor r11, r10
         | add r8, r11
         | rol r11, 21
         | xor r11, r8
         | add r10, r9
         | rol r9, 17
         | xor r9, r10
         | rol r10, 32
      end

      -- Add rax into state.
      local function process()
         | xor r11, rax
         for i=1,c do sipround() end
         | xor r8, rax
      end

      -- Compression phase.
      for i=1,size/8 do
         | mov rax, [rdi]
         | add rdi, 8
         process()
      end
      -- Load tail word and process it.
      if as_specified then
         | mov rax, size
         | shl rax, 56
         for i=1,size%8 do
            | movzx rdx, byte [rdi]
            | inc rdi
            if i > 1 then
              | shl rdx, ((i - 1) * 8)
            end
            | or rax, rdx
         end
         process()
      elseif size%8 ~= 0 then
         -- Fixed-size simplification: no need to add in size byte, we
         -- can use different byte orders if it's more convenient, and
         -- we don't have to do anything at all if the size is a
         -- multiple of 8.
         if size%8 >= 4 then
            | mov eax, [rdi]
            | add rdi, 4
         else
            | xor rax, rax
         end
         if size%4 >= 2 then
            | shl rax, 16
            | mov ax, [rdi]
            | add rdi, 2
         end
         if size%2 ~= 0 then
            | shl rax, 8
            | mov al, [rdi]
            | inc rdi
         end
         process()
      end
      -- Finalization.
      | xor r10, 0xff
      for i=1,d do sipround() end

      | mov rax, r8
      | xor rax, r9
      | xor rax, r10
      | xor rax, r11
      | shr rax, 32
      | ret
   end

   return assemble("make_sip_hash",
                   ffi.typeof("uint32_t (*)(uint8_t *)"),
                   gen)
end

local function SSE(stride)
   local asm = {}
   local Dst
   function asm.init(key)
      local initial_state = ffi.new('uint64_t[4]', load_initial_state(key))
      table.insert(__anchor, initial_state)
      | mov rax, initial_state
      | movddup xmm0, [rax]
      | movddup xmm1, [rax+8]
      | movddup xmm2, [rax+16]
      | movddup xmm3, [rax+24]
   end
   function asm.add(dst, other)
      | paddq xmm(dst), xmm(other)
   end
   function asm.shl(reg, bits)
      | psllq xmm(reg), bits
   end
   function asm.ior(dst, other)
      | por xmm(dst), xmm(other)
   end
   function asm.xor(dst, other)
      | pxor xmm(dst), xmm(other)
   end
   function asm.rol(reg, bits)
      | movupd xmm5, xmm(reg)
      | psllq xmm5, bits
      | psrlq xmm(reg), (64 - bits)
      | por xmm(reg), xmm5
   end
   function asm.load_u64_and_advance(dst)
      | movlpd xmm(dst), qword [rdi]
      | movhpd xmm(dst), qword [rdi+stride]
      | add rdi, 8
   end
   function asm.load_u32_and_advance(dst)
      | movss xmm(dst), dword [rdi]
      | pinsrd xmm(dst), dword [rdi + stride], 2
      | add rdi, 4
   end
   function asm.load_u16_and_advance(dst)
      | pxor xmm(dst), xmm(dst)
      | pinsrw xmm(dst), word [rdi], 0
      | pinsrw xmm(dst), word [rdi + stride], 4
      | add rdi, 2
   end
   function asm.load_u8_and_advance(dst)
      | pxor xmm(dst), xmm(dst)
      | pinsrb xmm(dst), byte [rdi], 0
      | pinsrb xmm(dst), byte [rdi + stride], 8
      | add rdi, 1
   end
   function asm.load_imm8(dst, imm)
      | xor rax, rax
      | mov al, imm
      | pinsrq xmm(dst), rax, 0
      | pinsrq xmm(dst), rax, 1
   end
   function asm.ret(reg)
      -- Extract high 32 bits from each 64-bit value to uint32_t[2] in
      -- rsi.
      | pextrd dword [rsi], xmm(reg), 1
      | pextrd dword [rsi+4], xmm(reg), 3
      | xor rax, rax
      | ret
   end
   function asm:assemble(name, gen)
      Dst = dasm.new(actions)
      gen(self)
      return finish(name.."_sse",
		    ffi.typeof("void (*)(uint8_t *, uint32_t *)"),
		    Dst)
   end
   return asm
end

local function AVX2(stride)
   local asm = {}
   local Dst
   function asm.init(key)
      local initial_state = ffi.new('uint64_t[4]', load_initial_state(key))
      table.insert(__anchor, initial_state)
      | vzeroupper
      | mov rax, initial_state
      | vbroadcastsd ymm0, qword [rax]
      | vbroadcastsd ymm1, qword [rax+8]
      | vbroadcastsd ymm2, qword [rax+16]
      | vbroadcastsd ymm3, qword [rax+24]
   end
   function asm.add(dst, other)
      | vpaddq ymm(dst), ymm(dst), ymm(other)
   end
   function asm.shl(reg, bits)
      | vpsllq ymm(reg), ymm(reg), bits
   end
   function asm.ior(dst, other)
      | vpor ymm(dst), ymm(dst), ymm(other)
   end
   function asm.xor(dst, other)
      | vpxor ymm(dst), ymm(dst), ymm(other)
   end
   function asm.rol(reg, bits)
      | vpsllq ymm5, ymm(reg), bits
      | vpsrlq ymm(reg), ymm(reg), (64 - bits)
      | vpor ymm(reg), ymm(reg), ymm5
   end
   function asm.load_u64_and_advance(dst)
      -- Though we could use a parallel load here, I understand that
      -- it decomposes into approximately the same number of uops as
      -- this.  Note that we load u64 words into DST in the
      -- interleaved order 0, 2, 1, 3; this makes the return sequence
      -- easier.
      assert(dst > 5)
      | vpinsrq xmm(dst), xmm(dst), qword [rdi], 0
      | vpinsrq xmm5, xmm5, qword [rdi+stride], 0
      | vpinsrq xmm(dst), xmm(dst), qword [rdi+stride*2], 1
      | vpinsrq xmm5, xmm5, qword [rdi+stride*3], 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 8
   end
   function asm.load_u32_and_advance(dst)
      assert(dst > 5)
      | mov eax, [rdi]
      | mov edx, [rdi+stride]
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm5, xmm5, rdx, 0
      | mov eax, [rdi+stride*2]
      | mov edx, [rdi+stride*3]
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vpinsrq xmm5, xmm5, rdx, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 4
   end
   function asm.load_u16_and_advance(dst)
      assert(dst > 5)
      | movzx rax, word [rdi]
      | movzx rdx, word [rdi+stride]
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm5, xmm5, rdx, 0
      | movzx rax, word [rdi+stride*2]
      | movzx rdx, word [rdi+stride*3]
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vpinsrq xmm5, xmm5, rdx, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 2
   end
   function asm.load_u8_and_advance(dst)
      assert(dst > 5)
      | movzx rax, byte [rdi]
      | movzx rdx, byte [rdi+stride]
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm5, xmm5, rdx, 0
      | movzx rax, byte [rdi+stride*2]
      | movzx rdx, byte [rdi+stride*3]
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vpinsrq xmm5, xmm5, rdx, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm5, 1
      | add rdi, 1
   end
   function asm.load_imm8(dst, imm)
      | xor rax, rax
      | mov al, imm
      | vpinsrq xmm(dst), xmm(dst), rax, 0
      | vpinsrq xmm(dst), xmm(dst), rax, 1
      | vinsertf128 ymm(dst), ymm(dst), xmm(dst), 1
   end
   function asm.ret(reg)
      -- Extract high 32 bits from each 64-bit value.
      | vextractf128 xmm5, ymm(reg), 1
      | vpunpckhdq xmm(reg), xmm(reg), xmm5
      -- Now write to uint32_t[4] in rsi.
      | vmovdqu oword [rsi], xmm(reg)
      | vzeroupper
      | ret
   end
   function asm:assemble(name, gen)
      Dst = dasm.new(actions)
      gen(self)
      return finish(name.."_avx2",
		    ffi.typeof("void (*)(uint8_t *, uint32_t *)"),
		    Dst)
   end
   return asm
end

local function Simulator()
   local asm = {}
   local input, output
   local state
   function asm.init(key)
      state = ffi.new('uint64_t[8]', load_initial_state(key))
   end
   function asm.add(dst, other)
      state[dst] = state[dst] + state[other]
   end
   function asm.shl(reg, bits)
      state[reg] = bit.lshift(state[reg], bits)
   end
   function asm.ior(dst, other)
      state[dst] = bit.bor(state[dst], state[other])
   end
   function asm.xor(dst, other)
      state[dst] = bit.bxor(state[dst], state[other])
   end
   function asm.rol(reg, bits)
      state[reg] = bit.rol(state[reg], bits)
   end
   function asm.load_u64_and_advance(dst)
      state[dst] = ffi.cast('uint64_t*', input)[0]
      input = input + 8
   end
   function asm.load_u32_and_advance(dst)
      state[dst] = ffi.cast('uint32_t*', input)[0]
      input = input + 4
   end
   function asm.load_u16_and_advance(dst)
      state[dst] = ffi.cast('uint16_t*', input)[0]
      input = input + 2
   end
   function asm.load_u8_and_advance(dst)
      state[dst] = input[0]
      input = input + 1
   end
   function asm.load_imm8(dst, imm)
      state[dst] = imm
   end
   function asm.ret(reg)
      output = tonumber(bit.rshift(state[reg], 32))
   end
   function asm:assemble(name, gen)
      return function(ptr)
	 input, output = ffi.cast('uint8_t*', ptr), nil
	 gen(self)
	 input = nil
	 return output
      end
   end
   return asm
end

function make_sip_hash_x(assembler, size, key, c, d, as_specified)
   c = c or 2
   d = d or 4
   function gen(asm)
      -- Arguments:
      -- rdi: packed keys as pointer
      -- rsi: output uint32_t[4]
      -- rax: scratch

      -- Registers used:
      -- ymm0-ymm3 map to SipHash variables v0-v3
      -- Other ymm registers are scratch.

      local function sipround()
         asm.add(0, 1)
         asm.rol(1, 13)
         asm.xor(1, 0)
         asm.rol(0, 32)
         asm.add(2, 3)
         asm.rol(3, 16)
         asm.xor(3, 2)
         asm.add(0, 3)
         asm.rol(3, 21)
         asm.xor(3, 0)
         asm.add(2, 1)
         asm.rol(1, 17)
         asm.xor(1, 2)
         asm.rol(2, 32)
      end

      local function process(input)
         asm.xor(3, input)
         for i=1,c do sipround() end
         asm.xor(0, input)
      end

      -- Initialization phase.
      asm.init(key)

      -- Compression phase.
      for i=1,size/8 do
         asm.load_u64_and_advance(6)
         process(6)
      end
      -- Load tail word and process it.
      if as_specified then
         asm.load_imm8(6, bit.band(size, 0xff))
         asm.shl(6, 56)
         for i=1,size%8 do
            asm.load_u8_and_advance(7)
            if i > 1 then asm.shl(7, (i - 1) * 8) end
            asm.ior(6, 7)
         end
         process(6)
      elseif size%8 ~= 0 then
         -- Fixed-size simplification: no need to add in size byte, we
         -- can use different byte orders if it's more convenient, and
         -- we don't have to do anything at all if the size is a
         -- multiple of 8.
         if size%8 >= 4 then
            asm.load_u32_and_advance(6)
         else
            asm.xor(6, 6)
         end
         if size%4 >= 2 then
            asm.load_u16_and_advance(7)
            asm.shl(6, 16)
            asm.ior(6, 7)
         end
         if size%2 ~= 0 then
            asm.load_u8_and_advance(7)
            asm.shl(6, 8)
            asm.ior(6, 7)
         end
         process(6)
      end

      -- Finalization.
      asm.load_imm8(6, 0xff)
      asm.xor(2, 6)
      for i=1,d do sipround() end
      asm.xor(0, 1)
      asm.xor(2, 3)
      asm.xor(0, 2)
      asm.ret(0)
   end

   return assembler(size):assemble("siphash_"..c.."_"..d.."_x", gen)
end

function make_sip_hash_x2(...)
   return make_sip_hash_x(SSE, ...)
end

function make_sip_hash_x4(...)
   return make_sip_hash_x(AVX2, ...)
end

function make_sip_hash_simulator(...)
   return make_sip_hash_x(Simulator, ...)
end

local function make_lua_sip_hash(size, key, c, d, as_specified)
   c = c or 2
   d = d or 4

   local function sipround(v0, v1, v2, v3)
      v0 = v0 + v1
      v1 = bit.rol(v1, 13)
      v1 = bit.bxor(v1, v0)
      v0 = bit.rol(v0, 32)
      v2 = v2 + v3
      v3 = bit.rol(v3, 16)
      v3 = bit.bxor(v3, v2)
      v0 = v0 + v3
      v3 = bit.rol(v3, 21)
      v3 = bit.bxor(v3, v0)
      v2 = v2 + v1
      v1 = bit.rol(v1, 17)
      v1 = bit.bxor(v1, v2)
      v2 = bit.rol(v2, 32)
      return v0, v1, v2, v3
   end

   local function process(m, v0, v1, v2, v3)
      v3 = bit.bxor(v3, m)
      for i=1,c do
         v0, v1, v2, v3 = sipround(v0, v1, v2, v3)
      end
      v0 = bit.bxor(v0, m)
      return v0, v1, v2, v3
   end

   return function(ptr)
      ptr = ffi.cast('uint8_t*', ptr)
      -- Initialization phase.
      local v0, v1, v2, v3 = load_initial_state(key)

      -- Compression phase.
      for i=1,size/8 do
         local m = ffi.cast('uint64_t*', ptr)[0]
         ptr = ptr + 8
         v0, v1, v2, v3 = process(m, v0, v1, v2, v3)
      end
      -- Process tail word.
      if as_specified then
         local tail = size + 0ULL
         tail = bit.lshift(tail, 56)
         for i=1,size%8 do
            local byte = ptr[0]
            ptr = ptr + 1
            tail = bit.bor(tail, bit.lshift(byte+0ULL, (i-1)*8))
         end
         v0, v1, v2, v3 = process(tail, v0, v1, v2, v3)
      elseif size%8 ~= 0 then
         -- Fixed-size simplification; see above.
         local tail = 0ULL
         if size%8 >= 4 then
            tail = tail + ffi.cast('uint32_t*', ptr)[0]
            ptr = ptr + 4
         end
         if size%4 >= 2 then
            tail = bit.lshift(tail, 16) + ffi.cast('uint16_t*', ptr)[0]
            ptr = ptr + 2
         end
         if size%2 ~= 0 then
            tail = bit.lshift(tail, 8) + ptr[0]
            ptr = ptr + 1
         end
         v0, v1, v2, v3 = process(tail, v0, v1, v2, v3)
      end
  
      -- Finalization.
      v2 = bit.bxor(v2, 0xffULL)
      -- SipHash-2-4 has 4 siprounds in the finalization phase.
      for i=1,d do
         v0, v1, v2, v3 = sipround(v0, v1, v2, v3)
      end
      return tonumber(bit.rshift(bit.bxor(v0, v1, v2, v3), 32))
   end
end

function selftest()
   -- test that the lua and dasm versions produce the same output
   local function test(size, key, c, d, as_specified)
      local dasm_hash = make_sip_hash(size, key, c, d, as_specified)
      local dasm_hash_x2 = make_sip_hash_x2(size, key, c, d, as_specified)
      local dasm_hash_x4 = make_sip_hash_x4(size, key, c, d, as_specified)
      local dasm_hash_simulator = make_sip_hash_simulator(size, key, c, d, as_specified)
      local lua_hash = make_lua_sip_hash(size, key, c, d, as_specified)

      for i=0, 255 do
         local buf = ffi.new("uint8_t[?]", size)
         local vbuf = ffi.new("uint8_t[?]", size*4)
         local vres = ffi.new("uint32_t[4]")
         for j=0, size-1 do
            buf[j] = (i + j) % size
            for stride=0,3 do
               vbuf[j + stride*size] = buf[j]
            end
         end
         local h1 = lua_hash(buf)
         local h2 = dasm_hash(buf)
         dasm_hash_x2(vbuf, vres)
	 local h3 = vres[0]
         dasm_hash_x4(vbuf, vres)
	 local h4 = vres[0]
         local h5 = dasm_hash_simulator(vbuf)
         -- TODO: Check elements of vres.
         if h1 ~= h2 then
            print(size, key, c, d, as_specified, h1, h2)
            error('dasm not equal to lua')
         end
         if h1 ~= h3 then
            print(size, key, c, d, as_specified, h1, h3)
            error('sse not equal to lua')
         end
         if h1 ~= h4 then
            print(size, key, c, d, as_specified, h1, h4)
            error('avx2 not equal to lua')
         end
         if h1 ~= h5 then
            print(size, key, c, d, as_specified, h1, h5)
            error('dasm simulator not equal to lua')
         end
      end
   end

   local key = reference_sip_hash_key()
   io.stdout:write("selftest: ")
   io.stdout:flush()
   for size=0,32 do
      io.stdout:write(".")
      io.stdout:flush()
      for c=0,2 do
         for d=0,4 do
            for _, as_specified in ipairs({true, false}) do
               test(size, key, c, d, as_specified)
            end
         end
      end
   end
   print("\nselftest ok")
end
